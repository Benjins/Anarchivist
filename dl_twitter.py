# Tweet Archiving Tool
# Usage: python dl_twitter.py {USERNAME1} {USERNAME2} ...
# 
# Archives an entire user's Twitter profile. It gets:
# - the text of the tweet
# - the time of the tweet
# - which tweet (if any) it is was replying to
# - Any photos and video in the tweet
# - Alt text of photos
# - *some* of their retweets (not all)
# 
# All data (except images + videos) is stored in a tweets.db sqlite database
# Images are stored at "Images/{USERNAME}/{TWEET_ID}_(IMAGE_INDEX}.{IMAGE_EXT}"
# Videos are stored under "Videos/{USERNAME}/{TWEET_ID}/", and contain multiple files generated by youtube-dl
#
# DEPENDENCIES:
# Uses Python 3
# Most of this script is self-contained. However, to download tweet videos, it 
# **assumes youtube-dl is installed and on your path.**
# FFmpeg may not be required, but it's recommended you also have that installed and on your path in case.
#
# Ethics: Please do not download other people's tweets for ill purposes. If someone does not wish
# you to download or repost their tweets, please respect that unless there is a very compelling reason not to.
#
# Notes/Caveats:
# - There are bugs, I have only tested this on my system
# - It is fairly fragile, expecting Twitter's HTML in a particular format. I will try to keep it up to date.
# - It is currently synchronous. I'd like to rewrite it to be async, but that will take some time
# - You can run multiple instances of the script, but *only if they are downloading different users.* There can be
#   some contention even then, but overall they shouldn't stomp on each other's toes.
#

import encodings.idna

import ssl

import sys
import os
import errno

import json

import http.client

from urllib.parse import urlencode

import sqlite3

import os
import sys
import time
import traceback

from subprocess import call

import re
        
def MaybeMakeDirectory(dirname):
    try:
        os.makedirs(dirname)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise    

db = sqlite3.connect('tweets.db', timeout=60.0)

# Set up Tweet Database
# TODO: Unique constraint on TweetImages/TweetVideo
db.execute('CREATE TABLE If NOT EXISTS TweetData(tweetID INTEGER PRIMARY KEY, username VARCHAR(255), data TEXT, timestamp VARCHAR(255), replyTo INTEGER);')
db.execute('CREATE TABLE If NOT EXISTS TweetImages(id INTEGER PRIMARY KEY AUTOINCREMENT, tweetID INTEGER, url VARCHAR(255), user VARCHAR(255), altText VARCHAR(255));')
db.execute('CREATE TABLE If NOT EXISTS TweetVideos(id INTEGER PRIMARY KEY AUTOINCREMENT, tweetID INTEGER, user VARCHAR(255));')
db.execute('CREATE TABLE If NOT EXISTS Retweets(id INTEGER PRIMARY KEY AUTOINCREMENT, tweetID INTEGER, origTweeter VARCHAR(255), retweeter VARCHAR(255));')
db.execute('CREATE INDEX If NOT EXISTS TweetIDIndex ON TweetData (tweetID);')
db.execute('CREATE INDEX If NOT EXISTS TweetDataUsernameIndex ON TweetData (username);')

db.commit()

#---------------------------------
    
class Redir301:
    url = ''
    def __init__(self, _url):
        self.url = _url

def GetNewHttpConn(origin, doSSL=True):
    conn = {}
    conn['origin'] = origin
    conn['doSSL'] = doSSL
    if conn['doSSL']:
        conn['conn'] = http.client.HTTPSConnection(conn['origin'], timeout=50)
    else:
        conn['conn'] = http.client.HTTPConnection(conn['origin'], timeout=50)
    return conn
    
def GetMessageWithRetries(conn, url, headersData={}, maxRetries = 13):
    timeout = 15
    for i in range(maxRetries):
        try:
            #print('Making request "%s"...' % url)
            conn['conn'].request("GET", url, headers=headersData)
            res = conn['conn'].getresponse()
            #print('Got response')
            
            #print('Got %d status...' % res.status, file=sys.stderr)
            #print(res.headers, file=sys.stderr)
            
            if res is not None and res.status == 200:
                data = res.read()
                #print('READ DATA')
                
                return data
            elif res.status == 303 or res.status == 302 or res.status == 301:
                data = res.read()
                
                print('Location: ' + res.getheader('Location'))
                
                return Redir301(res.getheader('Location'))
            elif res.status == 403:
                time.sleep(5.0)
                return None
            elif res.status == 404:
                return None
            else:
                print('Got status %d...' % res.status)
                time.sleep(timeout)
                timeout = timeout + 3
            
        except http.client.ResponseNotReady:
            print('Connection got ResponseNotReady, retrying...', file=sys.stderr)
            if conn['doSSL']:
                conn['conn'] = http.client.HTTPSConnection(conn['origin'], timeout=50)
            else:
                conn['conn'] = http.client.HTTPConnection(conn['origin'], timeout=50)
        except http.client.RemoteDisconnected:
            print('Connection got RemoteDisconnected, retrying...', file=sys.stderr)
            if conn['doSSL']:
                conn['conn'] = http.client.HTTPSConnection(conn['origin'], timeout=50)
            else:
                conn['conn'] = http.client.HTTPConnection(conn['origin'], timeout=50)
        except Exception as e:
            traceback.print_exc()
            print("Error, got exception: '%s'" % str(e), file=sys.stderr)
            conn['conn'] = http.client.HTTPSConnection(conn['origin'], timeout=50)
            time.sleep(timeout)
            timeout = timeout + 3

    return None


#--------------------------------------

IMGconn = GetNewHttpConn("pbs.twimg.com")
def DownloadURLIMG(url, headers={}):   
    return GetMessageWithRetries(IMGconn, url, headers)
        
TWconn = GetNewHttpConn("twitter.com")
def DownloadURL(url, headers={}, ):   
    return GetMessageWithRetries(TWconn, url, headers)

        
def GetTweetIDs(text):
    tweetIDs = []
    parts = text.split('data-item-id="')
    for part in parts[1:]:
        stuff = part.split('"',1)[0]
        
        tweetID = int(stuff)
        tweetIDs.append(tweetID)
        
    return tweetIDs
        
def DownloadTweets(user):
    print('Downloading all tweet ids from user "%s"' % user)
    params = {
        'f': 'tweets',
        'vertical': 'default',
        'lang': 'en',
        'q': 'from:%s' % user,
        'include_available_features': '1',
        'include_entities': '1',
        'reset_error_state': 'false',
        'src': 'typd',
        'qf': 'off'
    }
    
    url = '/i/search/timeline?' + urlencode(params)
    
    data = DownloadURL(url)
    while data is not None:
        text = data.decode('utf-8')
        
        info = json.loads(text)
        
        tweetIDs = GetTweetIDs(info['items_html'])
        
        if len(tweetIDs) <= 0:
            print('Got to end of search, no more tweets')
            break
        
        db.execute('BEGIN TRANSACTION;')
        for tweetID in tweetIDs:
            db.execute('INSERT OR IGNORE INTO TweetData VALUES(?,?,NULL,NULL,NULL)', (tweetID, user))
        db.execute('COMMIT;')
            
        db.commit()
        
        print('Added %d tweets...' % len(tweetIDs))
        
        # Previously, we used the has_more_items flag, but it seems to be wrong
        # An empty list is a better sentinel that the search has finished
        if True:#info['has_more_items']:
            minID = min(tweetIDs)
            maxID = max(tweetIDs)
            
            maxPos = 'TWEET-%d-%d' % (minID, maxID)
            params['max_position'] = maxPos
            
            url = '/i/search/timeline?' + urlencode(params)
            data = DownloadURL(url)
        else:
            break

reg = re.compile('<img data-aria-label-part src="https:\/\/pbs\.twimg\.com\/media\/(\w*\.(png|jpg))" *alt="(.*)"')

downloadedImgNames = {}
    
def GetImagesForTweet(tweetID, user, tweetPageHTML, imgDataToInsert):

    imgFilenameList = []

    for match in re.finditer(reg, tweetPageHTML):
        imgName = match.group(1)
        altText = match.group(3)
        try:
            print('On tweet %d, got Image: "%s" Alt text: "%s"' % (tweetID, imgName, altText))
        except:
            print('Unicode error maybe?')
        
        if imgName not in downloadedImgNames:
            imgFilenameList.append(imgName)
            downloadedImgNames[imgName] = 1
            
            imgDataToInsert.append( (tweetID,imgName,user,altText) ) 

    counter = 0
    for imgName in imgFilenameList:
        url = '/media/%s:large' % imgName
        filename = 'Images/%s/%d_%d.%s' % (user, tweetID, counter, imgName[-3:])
        if os.path.exists(filename):
            print('Skipping image "%s"' % imgName)
        else:
            data = DownloadURLIMG(url)
            if data is not None:
                print('Saving out to file "%s"' % filename)
                with open(filename, 'wb') as f:
                    f.write(data)
                
                counter += 1
            else:
                print('Could not download image "%s"...' % imgName)

def DownloadVideoForTweet(user, tweetID):
    MaybeMakeDirectory('Videos/%s' % user)
    
    folderName = 'Videos/%s/%d' % (user, tweetID)    
    MaybeMakeDirectory(folderName)
    
    url = 'https://twitter.com/%s/status/%d' % (user, tweetID)

    args = ['youtube-dl', '--continue', '--retries', '4', '--write-info-json', '--write-description', '--write-thumbnail', '--verbose', '--ignore-errors', '-o', folderName + '/' + '%(stitle)s.%(ext)s', url]
            
    call(args)
    
regDataItem = re.compile('data-item-id="([0-9]*)"')
    
def LookForReply(text, tweetID):
    if 'class="TweetTextSize TweetTextSize--jumb' not in text:
        print(':( No inline reply box...')
        return 0
    else:
        text = text.split('class="TweetTextSize TweetTextSize--jumb')[0]
        lastMatch = None
        for match in regDataItem.finditer(text):
            #print('mayyyybe reply ID: %d' % int(match.group(1)))
            if int(match.group(1)) != tweetID:
                lastMatch = match
            
        if lastMatch is None:
            return 0
        else:
            return int(lastMatch.group(1))

def DownloadRepliesTweetData():
    while True:
        dataToInsert = []
        cur = db.execute('select username,tweetID from TweetData WHERE data IS NOT NULL AND replyTo IS NULL LIMIT 100;')
        next = cur.fetchone()
        while next is not None:
            user = next[0]
            tweetID = next[1]
            
            url = '/%s/status/%d' % (user, tweetID)
            data = DownloadURL(url)
            if data is not None:
                text = data.decode('utf-8')
                
                replyID = LookForReply(text, tweetID)
                    
                print('Tweet %d from user "%s" replying to %d' % (tweetID, user, replyID))
                
                dataToInsert.append( (replyID, tweetID) )
            else:
                print('Error, couldn\'t download "%s"...' % url)
            
            next = cur.fetchone()
            
        if len(dataToInsert) > 0:
            db.execute('BEGIN TRANSACTION;')
            db.executemany('UPDATE TweetData SET replyTo=? WHERE tweetID=?;', dataToInsert)
            db.execute('COMMIT;')
            db.commit()
            
            dataToInsert = []


import re
            
redirReg = re.compile(r'https:\/\/twitter.com/([^\/]*)\/status\/([0-9]*)')

def DownloadTweetData(user):
    
    while True:
        # To help with concurrency, we keep a memory cache of all pending writes to the DB
        # in the format expected by executemany()
        dataToInsert = []
        vidDataToInsert = []
        imgDataToInsert = []
        dataToRemove = []
        rtsToInsert = []
        
        timeFetchStart = time.time()
        cur = db.execute('select tweetID from TweetData WHERE data IS NULL and username=? LIMIT 1000;', (user,))
        timeFetchEnd = time.time()
        print('Took %f seconds to fetch 1000 tweet ids...' % (timeFetchEnd - timeFetchStart))
        
        nextList = cur.fetchall()
        for next in nextList:
            tweetID = next[0]
            url = '/%s/status/%d' % (user, tweetID)
            data = DownloadURL(url)
            if data is not None and type(data) is Redir301:
                match = redirReg.match(data.url)
                if match:
                    print('Found RT: "%s" original tweeter' % match.group(1))
                    dataToRemove.append( (tweetID,) )
                    rtsToInsert.append( (tweetID, user, match.group(1)) )
                    
            elif data is not None:
                text = data.decode('utf-8')
                
                replyTo = LookForReply(text, tweetID)

                start = '<div class="js-tweet-text-container">'
                end = '</div>'
                
                parts = text.split(start)[1:]
                for part in parts:
                    tweetHTML = part.split(end, 1)[0]
                    if 'TweetTextSize--jumbo' in tweetHTML:
                        tweetHTML = tweetHTML.strip()
                        
                        timeParts = text.split('<span class="metadata">')[1].split('<span>')[1].split('</span>')[0]
                        
                        
                        dataToInsert.append( (tweetHTML, timeParts, replyTo, tweetID) )
                        
                        print(tweetID)
                        print(timeParts)
                        
                        break
                        
                textToLookForImagesIn = text.split('class="js-machine-translated-tweet-container"')[0].split('class="hidden-replies-container"')[-1]
                GetImagesForTweet(tweetID, user, textToLookForImagesIn, imgDataToInsert)
                
                if 'class="PlayableMedia-container"' in textToLookForImagesIn:
                    DownloadVideoForTweet(user, tweetID)
                    vidDataToInsert.append( (tweetID, user) )

        if len(dataToInsert) <= 0 and len(rtsToInsert) <= 0:
            break
        
        print('About to write out to DB')
        print('%d tweet texts, %d tweets\' images %d tweet\'s videos, %d rts, %d removes' % (len(dataToInsert), len(imgDataToInsert), len(vidDataToInsert), len(rtsToInsert), len(dataToRemove)))
        startTime = time.time()
        
        db.execute('BEGIN TRANSACTION;')
        db.executemany('UPDATE TweetData SET data=?,timestamp=?,replyTo=? WHERE tweetID=?;', dataToInsert)
        if len(vidDataToInsert) > 0:
            db.executemany('INSERT INTO TweetVideos(tweetID,user) VALUES(?,?);', vidDataToInsert)
        if len(imgDataToInsert) > 0:
            db.executemany('INSERT OR IGNORE INTO TweetImages(tweetID,url,user,altText) VALUES(?,?,?,?)', imgDataToInsert)
        if len(rtsToInsert) > 0:
            db.executemany('INSERT INTO Retweets(tweetID, retweeter, origTweeter) VALUES(?,?,?)', rtsToInsert)
        if len(dataToRemove) > 0:
            db.executemany('DELETE FROM TweetData WHERE tweetID=?', dataToRemove)
        db.execute('COMMIT;')
            
        db.commit()
        endTime = time.time()
        print('Finished writing in %f seconds' % (endTime - startTime))

    db.commit()
        

def DownloadUserTweets(user):
    DownloadTweets(user)
    MaybeMakeDirectory('Images/%s' % user)
    DownloadTweetData(user)

if len(sys.argv) <= 1:
    print('Please provide usernames as cmd line args')
else:
    for username in sys.argv[1:]:
        DownloadUserTweets(username.strip())

